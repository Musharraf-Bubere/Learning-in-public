{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da18cd6e",
   "metadata": {},
   "source": [
    "## ðŸ“ Projections of Vectors (Linear Algebra)\n",
    "\n",
    "![Vector Projection](images/Projections_of_Vector_Page_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Introduction to Vector Projections\n",
    "\n",
    "In linear algebra, **vector projection** helps us find how much of one vector lies **along the direction of another vector**.\n",
    "\n",
    "> **Projection of vector x onto a line L** means finding a vector that:\n",
    "- Lies **on the line L**\n",
    "- Is the **closest vector on L to x**\n",
    "- Makes the error vector **perpendicular** to the line\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Geometric Intuition (From the Diagram)\n",
    "\n",
    "From the figure:\n",
    "- **x** is an arbitrary vector\n",
    "- **L** is a line passing through the origin\n",
    "- **v** is a direction vector of the line L\n",
    "- **projâ‚—(x)** lies on line L\n",
    "- The vector **x âˆ’ projâ‚—(x)** is **perpendicular to L**\n",
    "\n",
    "This perpendicular condition is the key idea behind projections.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Orthogonality Condition\n",
    "\n",
    "Since the error vector is perpendicular to the line direction:\n",
    "\n",
    "\\[\n",
    "(x - c\\,v) \\cdot v = 0\n",
    "\\]\n",
    "\n",
    "Expanding:\n",
    "\n",
    "\\[\n",
    "x \\cdot v - c(v \\cdot v) = 0\n",
    "\\]\n",
    "\n",
    "Solving for **c**:\n",
    "\n",
    "\\[\n",
    "c = \\frac{x \\cdot v}{v \\cdot v}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Projection Formula (Important)\n",
    "\n",
    "The **projection of vector x onto line L** is:\n",
    "\n",
    "\\[\n",
    "\\boxed{\n",
    "\\text{proj}_L(x) = \\left(\\frac{x \\cdot v}{v \\cdot v}\\right)v\n",
    "}\n",
    "\\]\n",
    "\n",
    "This formula is valid **only when the line passes through the origin**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Dot Product Reminder\n",
    "\n",
    "For vectors:\n",
    "\n",
    "\\[\n",
    "a =\n",
    "\\begin{bmatrix}\n",
    "a_1 \\\\ a_2\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "b =\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\ b_2\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Dot product:\n",
    "\n",
    "\\[\n",
    "a \\cdot b = a_1b_1 + a_2b_2\n",
    "\\]\n",
    "\n",
    "If:\n",
    "\\[\n",
    "a \\cdot b = 0\n",
    "\\]\n",
    "then the vectors are **orthogonal (perpendicular)**.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Worked Example (From Page 2)\n",
    "\n",
    "![Vector Projection Example](images/Projections_of_Vector_Page_2.png)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Given\n",
    "\n",
    "Line **L** is spanned by vector:\n",
    "\n",
    "\\[\n",
    "v =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Vector to be projected:\n",
    "\n",
    "\\[\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\ 3\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Step 1: Compute Dot Products\n",
    "\n",
    "### Dot product \\(x \\cdot v\\)\n",
    "\n",
    "\\[\n",
    "x \\cdot v = (2)(2) + (3)(1) = 4 + 3 = 7\n",
    "\\]\n",
    "\n",
    "### Dot product \\(v \\cdot v\\)\n",
    "\n",
    "\\[\n",
    "v \\cdot v = (2)^2 + (1)^2 = 4 + 1 = 5\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Step 2: Compute Projection Scalar\n",
    "\n",
    "\\[\n",
    "\\frac{x \\cdot v}{v \\cdot v} = \\frac{7}{5}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Step 3: Compute Projection Vector\n",
    "\n",
    "\\[\n",
    "\\text{proj}_L(x) = \\frac{7}{5}\n",
    "\\begin{bmatrix}\n",
    "2 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{14}{5} \\\\ \\frac{7}{5}\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final Answer\n",
    "\n",
    "\\[\n",
    "\\boxed{\n",
    "\\text{proj}_L(x) =\n",
    "\\begin{bmatrix}\n",
    "2.8 \\\\ 1.4\n",
    "\\end{bmatrix}\n",
    "}\n",
    "\\]\n",
    "\n",
    "This is the **shadow of vector x on the line L**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Interpretation\n",
    "\n",
    "- The projection lies **on the line**\n",
    "- The error vector is **perpendicular to the line**\n",
    "- Projection minimizes the distance between **x** and the line\n",
    "- Widely used in:\n",
    "  - Least Squares\n",
    "  - Machine Learning\n",
    "  - Optimization\n",
    "  - Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Key Takeaways\n",
    "\n",
    "- Vector projection decomposes a vector into:\n",
    "  - Parallel component\n",
    "  - Perpendicular component\n",
    "- Projection formula:\n",
    "  \\[\n",
    "  \\text{proj}_L(x) = \\left(\\frac{x \\cdot v}{v \\cdot v}\\right)v\n",
    "  \\]\n",
    "- Dot product zero â‡’ vectors are orthogonal\n",
    "- Projections are foundational in **data science & ML**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
